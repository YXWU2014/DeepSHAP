{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss ablation for an NHANES model\n",
    "\n",
    "**Question of interest: Does loss ablation improve for loss explanations in comparison to output explanations**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC  0.9882752748310369\n",
      "Test ROC AUC   0.8681190191906044\n",
      "Train loss     0.08771404129969228\n",
      "Test loss      0.26650387368177847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import pickle\n",
    "import shap\n",
    "import os\n",
    "\n",
    "from deepshap.data import load_nhanes_new\n",
    "\n",
    "# Name of experiment we are running\n",
    "exp_name = \"cycle_shift\"\n",
    "\n",
    "# Load data\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = load_nhanes_new()\n",
    "\n",
    "# Train or load model\n",
    "mpath = \"models/{}_xgb.p\".format(exp_name)\n",
    "if not os.path.exists(mpath):\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], \n",
    "              early_stopping_rounds=10, verbose=False)\n",
    "    pickle.dump(model, open(mpath,\"wb\"))\n",
    "else:\n",
    "    model = pickle.load(open(mpath,\"rb\"))\n",
    "    \n",
    "# Evaluate model performance\n",
    "print(\"Train ROC AUC \", roc_auc_score(y_train, model.predict_proba(X_train)[:,1]))\n",
    "print(\"Test ROC AUC  \", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n",
    "print(\"Train loss    \", log_loss(y_train, model.predict_proba(X_train)[:,1]))\n",
    "print(\"Test loss     \", log_loss(y_test, model.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/compute attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepshap.explanation import lime_wrapper\n",
    "import pandas as pd\n",
    "\n",
    "def log_loss_per_sample(ytrue, ypred):\n",
    "    \"\"\"\n",
    "    log_loss computation that does not fail if all labels are the same\n",
    "    \"\"\"\n",
    "    return((-(ytrue*np.log(ypred)+(1-ytrue)*np.log(1-ypred))))\n",
    "\n",
    "def log_loss_safe(ytrue, ypred):\n",
    "    \"\"\"\n",
    "    log_loss computation that does not fail if all labels are the same\n",
    "    \"\"\"\n",
    "    return(log_loss_per_sample(ytrue, ypred))\n",
    "\n",
    "def loss_fn_neg(x): \n",
    "    x_df = pd.DataFrame(x, columns=X_train.columns)\n",
    "    return(log_loss_safe(np.zeros(x.shape[0]), model.predict_proba(x_df)[:,1]))\n",
    "\n",
    "def loss_fn_pos(x): \n",
    "    x_df = pd.DataFrame(x, columns=X_train.columns)\n",
    "    return(log_loss_safe(np.ones(x.shape[0]), model.predict_proba(x_df)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_explic = X_train[0:1000]\n",
    "y_explic = y_train[0:1000]\n",
    "# Need to repeat to same shape as x_explic\n",
    "\n",
    "np.random.seed(102190)\n",
    "rand_inds = np.random.choice(range(X_train.shape[0]), 1000, replace=False)\n",
    "reference = X_train.iloc[rand_inds]\n",
    "\n",
    "x_explic_pos = x_explic[y_explic==1]\n",
    "y_explic_pos = y_explic[y_explic==1]\n",
    "\n",
    "x_explic_neg = x_explic[y_explic==0]\n",
    "y_explic_neg = y_explic[y_explic==0]\n",
    "\n",
    "loss_pos = loss_fn_pos(x_explic_pos)\n",
    "loss_neg = loss_fn_neg(x_explic_neg)\n",
    "\n",
    "# Verify that the mean loss matches\n",
    "np.allclose(np.hstack([loss_pos,loss_neg]).mean(),\n",
    "            log_loss(y_explic, model.predict_proba(x_explic)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute or load attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_attr_name   = \"attr/{}_train1000_loss_attr_deep.npy\".format(exp_name)\n",
    "sample_attr_name = \"attr/{}_train1000_loss_attr_sampling.npy\".format(exp_name)\n",
    "kernel_attr_name = \"attr/{}_train1000_loss_attr_kernel.npy\".format(exp_name)\n",
    "lime_attr_name   = \"attr/{}_train1000_loss_attr_lime.npy\".format(exp_name)\n",
    "runtimes_name    = \"attr/{}_expl_times_1000.p\".format(exp_name)\n",
    "attr_names = [deep_attr_name, sample_attr_name, kernel_attr_name, lime_attr_name, runtime_names]\n",
    "\n",
    "if all([os.path.exists(name) for name in attr_names]):\n",
    "    # DeepSHAP attributions\n",
    "    start = time()\n",
    "    loss_explainer = shap.TreeExplainer(model, data=reference, model_output=\"log_loss\")\n",
    "    attr_deep = loss_explainer.shap_values(X_train[0:1000], y_train[0:1000])\n",
    "    deep_expl_time = time() - start\n",
    "    np.save(deep_attr_name, attr_deep)\n",
    "\n",
    "    # SamplingExplainer attributions\n",
    "    start = time()\n",
    "    explanation_fn = shap.SamplingExplainer\n",
    "    explainer_pos  = explanation_fn(loss_fn_pos, reference)\n",
    "    attr_pos       = explainer_pos.shap_values(x_explic_pos, num_samples=1000)\n",
    "\n",
    "    explainer_neg  = explanation_fn(loss_fn_neg, reference)\n",
    "    attr_neg       = explainer_neg.shap_values(x_explic_neg, num_samples=1000)\n",
    "    sampling_expl_time = time() - start\n",
    "\n",
    "    attr_sample = np.zeros([1000,153])\n",
    "    attr_sample[y_train[0:1000]==1] = attr_pos\n",
    "    attr_sample[y_train[0:1000]==0] = attr_neg\n",
    "    np.save(sample_attr_name, attr_sample)\n",
    "\n",
    "    # KernelExplainer attributions\n",
    "    start = time()\n",
    "    explanation_fn = shap.KernelExplainer\n",
    "    explainer_pos  = explanation_fn(loss_fn_pos, reference)\n",
    "    attr_pos       = explainer_pos.shap_values(x_explic_pos, num_samples=1000)\n",
    "\n",
    "    explainer_neg  = explanation_fn(loss_fn_neg, reference)\n",
    "    attr_neg       = explainer_neg.shap_values(x_explic_neg, num_samples=1000)\n",
    "    kernel_expl_time = time() - start\n",
    "\n",
    "    attr_kernel = np.zeros([1000,153])\n",
    "    attr_kernel[y_train[0:1000]==1] = attr_pos\n",
    "    attr_kernel[y_train[0:1000]==0] = attr_neg\n",
    "    np.save(kernel_attr_name, attr_kernel)\n",
    "\n",
    "    # LIME attributions\n",
    "    start = time()\n",
    "    attr_pos = lime_wrapper(loss_fn_pos, x_explic_pos.values, \n",
    "                            reference, mode=\"regression\", \n",
    "                            wrap_pred=False, num_samples=1000)\n",
    "\n",
    "    attr_neg = lime_wrapper(loss_fn_neg, x_explic_neg.values, \n",
    "                            reference, mode=\"regression\", \n",
    "                            wrap_pred=False, num_samples=1000)\n",
    "    lime_expl_time = time() - start\n",
    "\n",
    "    attr_lime = np.zeros([1000,153])\n",
    "    attr_lime[y_train[0:1000]==1] = attr_pos\n",
    "    attr_lime[y_train[0:1000]==0] = attr_neg\n",
    "    np.save(lime_attr_name, attr_lime)\n",
    "\n",
    "    expl_times = {\"deep\":deep_expl_time, \"sample\":sampling_expl_time, \n",
    "                  \"kernel\":kernel_expl_time, \"lime\":lime_expl_time}\n",
    "    pickle.dump(expl_times, open(runtimes_name, \"wb\"))\n",
    "else:\n",
    "    attr_deep   = np.load(deep_attr_name)\n",
    "    attr_sample = np.load(sample_attr_name)\n",
    "    attr_kernel = np.load(kernel_attr_name)\n",
    "    attr_lime   = np.load(lime_attr_name)\n",
    "    \n",
    "    expl_times  = pickle.load(open(runtimes_name, \"rb\"))\n",
    "    deep_expl_time     = expl_times[\"deep\"]\n",
    "    sampling_expl_time = expl_times[\"sample\"]\n",
    "    kernel_expl_time   = expl_times[\"kernel\"]\n",
    "    lime_expl_time     = expl_times[\"lime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# DeepSHAP attributions\n",
    "start = time()\n",
    "loss_explainer = shap.TreeExplainer(model, data=reference, model_output=\"log_loss\")\n",
    "attr_deep = loss_explainer.shap_values(X_train[0:1000], y_train[0:1000])\n",
    "deep_expl_time = time() - start\n",
    "\n",
    "np.save(\"attr/{}_train1000_loss_attr_deep.npy\".format(exp_name), attr_deep)\n",
    "\n",
    "# SamplingExplainer attributions\n",
    "start = time()\n",
    "explanation_fn = shap.SamplingExplainer\n",
    "explainer_pos  = explanation_fn(loss_fn_pos, reference)\n",
    "attr_pos       = explainer_pos.shap_values(x_explic_pos, num_samples=1000)\n",
    "\n",
    "explainer_neg  = explanation_fn(loss_fn_neg, reference)\n",
    "attr_neg       = explainer_neg.shap_values(x_explic_neg, num_samples=1000)\n",
    "sampling_expl_time = time() - start\n",
    "\n",
    "attr_sample = np.zeros([1000,153])\n",
    "attr_sample[y_train[0:1000]==1] = attr_pos\n",
    "attr_sample[y_train[0:1000]==0] = attr_neg\n",
    "np.save(\"attr/{}_train1000_loss_attr_sampling.npy\".format(exp_name), attr_sample)\n",
    "\n",
    "# KernelExplainer attributions\n",
    "start = time()\n",
    "explanation_fn = shap.KernelExplainer\n",
    "explainer_pos  = explanation_fn(loss_fn_pos, reference)\n",
    "attr_pos       = explainer_pos.shap_values(x_explic_pos, num_samples=1000)\n",
    "\n",
    "explainer_neg  = explanation_fn(loss_fn_neg, reference)\n",
    "attr_neg       = explainer_neg.shap_values(x_explic_neg, num_samples=1000)\n",
    "kernel_expl_time = time() - start\n",
    "\n",
    "attr_kernel = np.zeros([1000,153])\n",
    "attr_kernel[y_train[0:1000]==1] = attr_pos\n",
    "attr_kernel[y_train[0:1000]==0] = attr_neg\n",
    "np.save(\"attr/{}_train1000_loss_attr_kernel.npy\".format(exp_name), attr_kernel)\n",
    "\n",
    "# LIME attributions\n",
    "start = time()\n",
    "attr_pos = lime_wrapper(loss_fn_pos, x_explic_pos.values, \n",
    "                        reference, mode=\"regression\", \n",
    "                        wrap_pred=False, num_samples=1000)\n",
    "\n",
    "attr_neg = lime_wrapper(loss_fn_neg, x_explic_neg.values, \n",
    "                        reference, mode=\"regression\", \n",
    "                        wrap_pred=False, num_samples=1000)\n",
    "lime_expl_time = time() - start\n",
    "\n",
    "attr_lime = np.zeros([1000,153])\n",
    "attr_lime[y_train[0:1000]==1] = attr_pos\n",
    "attr_lime[y_train[0:1000]==0] = attr_neg\n",
    "np.save(\"attr/{}_train1000_loss_attr_lime.npy\".format(exp_name), attr_lime)\n",
    "\n",
    "expl_times = {\"deep\":deep_expl_time, \"sample\":sampling_expl_time, \n",
    "              \"kernel\":kernel_expl_time, \"lime\":lime_expl_time}\n",
    "pickle.dump(expl_times, open(\"attr/{}_expl_times_1000.p\".format(exp_name), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Get output attributions ###\n",
    "###############################\n",
    "train_attr_path = \"attr/{}_train_attr.npy\".format(exp_name)\n",
    "test_attr_path  = \"attr/{}_test_attr.npy\".format(exp_name)\n",
    "\n",
    "np.random.seed(102190)\n",
    "rand_inds = np.random.choice(range(X_train.shape[0]), 1000, replace=False)\n",
    "reference = X_train.iloc[rand_inds]\n",
    "\n",
    "explainer = shap.TreeExplainer(model, data=reference)\n",
    "if not (os.path.exists(train_attr_path) and os.path.exists(test_attr_path)):\n",
    "    train_attr = explainer.shap_values(X_train)\n",
    "    test_attr  = explainer.shap_values(X_test)\n",
    "    \n",
    "    np.save(train_attr_path, train_attr)\n",
    "    np.save(test_attr_path, test_attr)\n",
    "else:\n",
    "    train_attr = np.load(train_attr_path)\n",
    "    test_attr  = np.load(test_attr_path)\n",
    "\n",
    "#############################\n",
    "### Get loss attributions ###\n",
    "#############################\n",
    "train_loss_attr_path = \"attr/{}_train_loss_attr.npy\".format(exp_name)\n",
    "test_loss_attr_path  = \"attr/{}_test_loss_attr.npy\".format(exp_name)\n",
    "\n",
    "loss_explainer = shap.TreeExplainer(model, data=reference, model_output=\"log_loss\")\n",
    "if not (os.path.exists(train_loss_attr_path) and os.path.exists(test_loss_attr_path)):\n",
    "    train_loss_attr = loss_explainer.shap_values(X_train, y_train)\n",
    "    test_loss_attr  = loss_explainer.shap_values(X_test, y_test)\n",
    "    \n",
    "    np.save(train_loss_attr_path, train_loss_attr)\n",
    "    np.save(test_loss_attr_path, test_loss_attr)\n",
    "else:\n",
    "    train_loss_attr = np.load(train_loss_attr_path)\n",
    "    test_loss_attr  = np.load(test_loss_attr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_attr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9eb69471e4fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m### Ablate OUTPUT attributions ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m##################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m### Positive ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_attr' is not defined"
     ]
    }
   ],
   "source": [
    "from deepshap.evaluation import ablate\n",
    "import pandas as pd\n",
    "plt.rcParams['figure.figsize'] = 4,4\n",
    "\n",
    "def loss_fn(x, y): \n",
    "    x_df = pd.DataFrame(x, columns=x_explic.columns)\n",
    "    return(log_loss(y, model.predict_proba(x_df)[:,1]))\n",
    "\n",
    "x_explic = X_train[0:1000]\n",
    "y_explic = y_train[0:1000]\n",
    "# Need to repeat to same shape as x_explic\n",
    "refer = np.repeat(np.array(x_explic.mean(0))[np.newaxis,:],1000,0) \n",
    "\n",
    "##################################\n",
    "### Ablate OUTPUT attributions ###\n",
    "##################################\n",
    "attr = train_attr[0:1000]\n",
    "\n",
    "### Positive ###\n",
    "ablated_loss = ablate(loss_fn, attr, x_explic, impute=\"pos\", \n",
    "                      refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss, linewidth=3)\n",
    "plt.title(\"Ablate Positive Output\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "plt.xlabel(\"Number of Features Ablated\")\n",
    "plt.savefig(\"fig/{}_ablate_pos_out.pdf\".format(exp_name))\n",
    "plt.show()\n",
    "\n",
    "### Negative ###\n",
    "ablated_loss = ablate(loss_fn, attr, x_explic, impute=\"neg\", \n",
    "                      refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss, linewidth=3)\n",
    "plt.title(\"Ablate Negative Output\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "plt.xlabel(\"Number of Features Ablated\")\n",
    "plt.savefig(\"fig/{}_ablate_neg_out.pdf\".format(exp_name))\n",
    "plt.show()\n",
    "\n",
    "################################\n",
    "### Ablate LOSS attributions ###\n",
    "################################\n",
    "GREEN  = \"#1ea81e\"\n",
    "BLUE   = \"#1e88e5\"\n",
    "RED    = \"#ff0d57\"\n",
    "ORANGE = \"#ff8800\"\n",
    "\n",
    "### Positive ###\n",
    "ablated_loss_deep = ablate(loss_fn, attr_deep, x_explic, impute=\"pos\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_deep, linewidth=3, label=\"DeepSHAP\", color=BLUE)\n",
    "ablated_loss_sample = ablate(loss_fn, attr_sample, x_explic, impute=\"pos\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_sample, linewidth=3, label=\"IME\", color=ORANGE)\n",
    "ablated_loss_kernel = ablate(loss_fn, attr_kernel, x_explic, impute=\"pos\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_kernel, linewidth=3, label=\"Kernel\", color=GREEN)\n",
    "ablated_loss_lime = ablate(loss_fn, attr_lime, x_explic, impute=\"pos\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_lime, linewidth=3, label=\"LIME\", color=RED)\n",
    "\n",
    "plt.title(\"Ablate Positive Loss\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "plt.xlabel(\"Number of Features Ablated\")\n",
    "plt.legend()\n",
    "plt.savefig(\"fig/{}_ablate_pos_loss.pdf\".format(exp_name))\n",
    "plt.show()\n",
    "\n",
    "### Negative ###\n",
    "ablated_loss_deep = ablate(loss_fn, attr_deep, x_explic, impute=\"neg\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_deep, linewidth=3, label=\"DeepSHAP\", color=BLUE)\n",
    "ablated_loss_sample = ablate(loss_fn, attr_sample, x_explic, impute=\"neg\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_sample, linewidth=3, label=\"IME\", color=ORANGE)\n",
    "ablated_loss_kernel = ablate(loss_fn, attr_kernel, x_explic, impute=\"neg\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_kernel, linewidth=3, label=\"Kernel\", color=GREEN)\n",
    "ablated_loss_lime = ablate(loss_fn, attr_lime, x_explic, impute=\"neg\", \n",
    "                             refer=refer, y_explic=y_explic, is_loss=True)\n",
    "plt.plot(ablated_loss_lime, linewidth=3, label=\"LIME\", color=RED)\n",
    "\n",
    "# ablated_loss = ablate(loss_fn, attr, x_explic, impute=\"neg\", \n",
    "#                       refer=refer, y_explic=y_explic, is_loss=True)\n",
    "# plt.plot(ablated_loss, linewidth=3)\n",
    "plt.title(\"Ablate Negative Loss\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "plt.xlabel(\"Number of Features Ablated\")\n",
    "plt.legend()\n",
    "plt.savefig(\"fig/{}_ablate_neg_loss.pdf\".format(exp_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 2,4\n",
    "bar = plt.bar(np.arange(4),[deep_expl_time, sampling_expl_time, kernel_expl_time, lime_expl_time],\n",
    "              color = [BLUE, ORANGE, GREEN, RED])\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, '%d' % int(height), ha='center', va='bottom')\n",
    "\n",
    "plt.ylim([-5,15500])\n",
    "    \n",
    "plt.xticks(np.arange(4),[\"DeepSHAP\", \"IME\", \"Kernel\", \"LIME\"], rotation=-20)\n",
    "plt.ylabel(\"Run time (seconds)\")\n",
    "plt.title(\"Runtime comparison\")\n",
    "plt.savefig(\"fig/{}_ablate_loss_runtime.pdf\".format(exp_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
